<h1 align="center"> Video-Question-Answering-Resources </h1>

The Video-Question-Answering-Resources repository is a curated guide for both beginners and researchers interested in the field of Video Question Answering (VQA). It provides an organized collection of the most relevant papers, models, datasets, and additional resources to help users understand and contribute to this evolving area. The repository focuses on the intersection of computer vision and natural language processing, particularly how video data can be used to answer complex questions, offering a range of materials from introductory guides to advanced research. (Last Update on 10/11/2024)

### Keywords:

### Curators:
[ Bharatesh Chakravarthi, Ph.D](https://chakravarthi589.github.io/)
</br>
[Joseph Raj Vishal](https://github.com/joe-rabbit)

---


- [**Beginners Guide to Video-Question-Answering**](#Beginners-Guide-to-Video-Question-Answering) <br>
- [**Publications**](#Publications) <br/>
  - Survey/Review Papers
  - Conference/Journal Papers 
- [**Benchmark Datasets**](#Benchmark-Datasets) <br>
- [**Current Models on Hugging Face**](#Current-Models-on-Hugging-Face) <br>
- [**Additional Resources**](#Additional-Resourcese) <br>

  
---
## Beginners Guide to Video Question Answering

1. **[Answering Questions from YouTube Videos with OpenAI Whisper and GPT-4 (Medium article)](https://medium.com/@mksupriya2/answering-questions-from-youtube-videos-with-openai-whisper-and-gpt-4-9a0ae11389ba)**

2. **[Try a quick example on how to use LLMs for Video Question Answering here](https://colab.research.google.com/drive/1qTUr1rYB3L3ZlFyLocWbRKg_HVfLvyvT?usp=sharing)** (Check Additional Resources for API key)
3.  **[Community  Computer Vision Course (Unit 4) MultiModal Models](https://huggingface.co/learn/computer-vision-course/en/unit4/multimodal-models/vlm-intro)**

## Publications 

---

---
## Datasets
| Name | Features | Link |
|------|----------|------|
|      |          |      |
---
## Models
## Open Source Models
| Model Name  | Links |
|-------------|-------------------------------|
| [InternVL](https://huggingface.co/OpenGVLab) | [Hugging Face](https://huggingface.co/OpenGVLab/InternVL2-76B) , [GitHub](https://github.com/OpenGVLab/InternVL) |
| [LLaVa](https://llava-vl.github.io/) | [Hugging Face](https://huggingface.co/docs/transformers/en/model_doc/llava) , [GitHub](https://github.com/haotian-liu/LLaVA) |
| [LITA](https://github.com/NVlabs/LITA) | [GitHub](https://github.com/NVlabs/LITA)|
|[End2End ChatBot](https://github.com/OpenGVLab/Ask-Anything/tree/main)|[Hugging Face](https://huggingface.co/spaces/OpenGVLab/InternVideo2-Chat-8B-HD) , [GitHub](https://github.com/OpenGVLab/Ask-Anything)|
|[VideoLLAMA2](https://huggingface.co/collections/DAMO-NLP-SG/videollama-2-6669b6b6f0493188305c87ed) | [Hugging Face](https://github.com/DAMO-NLP-SG/VideoLLaMA2) , [GitHub](https://huggingface.co/spaces/lixin4ever/VideoLLaMA2)|

---

## Models that Require APIs
| Model Name | API Link |
|------------|----------|
| ChatGPT    | [Here](https://platform.openai.com/api-keys) |
| Gemini |[Here](https://ai.google.dev/gemini-api/docs/vision?lang=python)|
| Llama 3.2|[Here](https://docs.llama-api.com/quickstart#llama-3-2-instruct-chat-models-with-vision)|


---

## Additional Resources

1. **[OpenAI Docs](https://platform.openai.com/docs/api-reference/introduction)**
2. **[Gemini Docs](https://ai.google.dev/gemini-api/docs)**
3. **[LLAMA Docs](https://docs.llama-api.com/quickstart)**
---

### :arrow_heading_up: [Back to Top](#Keywords)




